import sqlite3
import numpy as np
import pandas as pd
from tensorflow.keras.preprocessing.sequence import pad_sequences

# -------------------- 1. åƒæ•¸é…ç½® --------------------
config = {
    "db_path": "mt5_data.db",        # SQLite æ•¸æ“šåº«è·¯å¾‘
    "table_name": "main.XAUUSD",     # æ•¸æ“šè¡¨åç¨±
    "initial_input_length": 20,      # æœ€çŸ­çš„è¼¸å…¥é•·åº¦
    "max_input_length": 30,          # æœ€å¤§çš„è¼¸å…¥é•·åº¦ (padding ç”¨)
    "initial_forecast_length": 10,   # æœ€åˆçš„é æ¸¬é•·åº¦
    "min_forecast_length": 3,        # æœ€å°çš„é æ¸¬é•·åº¦
    "num_samples_to_visualize": 5,   # å¯è¦–åŒ–çš„æ¨£æœ¬æ•¸
    "train_ratio": 0.8               # è¨“ç·´é›†ä½”æ¯” (ç¾¤çµ„å±¤é¢)
}


# -------------------- 2. è®€å–æ•¸æ“š --------------------
def load_mt5_data(db_path, table_name):
    """
    å¾ SQLite æ•¸æ“šåº«è®€å– OHLCV æ•¸æ“šï¼Œè½‰æ›ç‚º Pandas DataFrameã€‚
    """
    conn = sqlite3.connect(db_path)
    query = f"SELECT time, open, high, low, close, tick_volume FROM {table_name} ORDER BY time ASC"
    df = pd.read_sql(query, conn)
    conn.close()

    # è½‰æ›æ™‚é–“æ ¼å¼
    df['time'] = pd.to_datetime(df['time'])
    return df


# -------------------- 3. ç”Ÿæˆè³‡æ–™ + åˆ†ç¾¤ + åˆ†å‰² --------------------
def prepare_data_and_split(df, config):
    """
    1) æŒ‰ç…§å‹•æ…‹çª—å£ç”¢ç”Ÿ (X, y, mask)
    2) çµ¦æ¯ç­†æ¨£æœ¬ä¸€å€‹ group_id
    3) è‹¥æœ€å¾Œä¸€çµ„æœªå®Œæˆ (å°šæœªé”åˆ°çµæŸæ¢ä»¶)ï¼Œå‰‡å°‡è©²æœªå®Œæˆçš„ç¾¤çµ„æ’é™¤
    4) æœ€å¾Œä¾ group_id åˆ‡åˆ† train/test
    """
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    X_list, y_list, mask_list, group_id_list = [], [], [], []
    y_mask_list = []

    data = df[['open', 'high', 'low', 'close', 'tick_volume']].values
    total_length = len(data)

    input_length = config["initial_input_length"]
    forecast_length = config["initial_forecast_length"]
    max_input_length = config["max_input_length"]
    min_forecast_length = config["min_forecast_length"]

    start_index = 0
    current_group_id = 0  # å¾ 0 é–‹å§‹ç·¨ç¾¤
    last_completed_group_id = -1  # *** æ–°å¢ï¼šç”¨ä¾†ç´€éŒ„æœ€å¾ŒçµæŸçš„ç¾¤çµ„

    while start_index + input_length + forecast_length <= total_length:
        X = data[start_index: start_index + input_length]
        y = data[start_index + input_length: start_index + input_length + forecast_length, 3]  # close
        mask = np.ones((input_length, 1))

        # - ç”¢ç”Ÿ y_mask: shape=(forecast_length,)ï¼Œéƒ½ç‚º 1
        #   å¾ŒçºŒæœƒå† pad åˆ° (max_forecast_length,)ã€‚
        #   ç•¶ç„¶ï¼Œä¹Ÿå¯ä»¥å…ˆå»ºç«‹æˆ shape=(max_forecast_length,)ï¼Œå‰få€‹1ã€å¾Œé¢0
        #   çœ‹ä½  padding æ–¹å¼è€Œå®šã€‚é€™è£¡å…ˆç”¢ç”Ÿ shape=(f,)
        y_mask_unpadded = np.ones((forecast_length,), dtype='float32')

        X_list.append(X)
        y_list.append(y)
        mask_list.append(mask)
        group_id_list.append(current_group_id)
        # å…ˆå­˜ä¸‹é€™å€‹ unpadded mask
        y_mask_list.append(y_mask_unpadded)

        # å‹•æ…‹èª¿æ•´ input_length/forecast_length
        if forecast_length > min_forecast_length:
            input_length += 1
            forecast_length -= 1
        else:
            # ä»£è¡¨é€™å€‹ group å·²å®Œæˆï¼Œé–‹å§‹æ–°çš„ä¸€çµ„
            last_completed_group_id = current_group_id  # *** æ–°å¢ï¼šæˆåŠŸçµæŸç¾¤çµ„å¾Œæ›´æ–°
            current_group_id += 1
            # é‡ç½®è¼¸å…¥é•·åº¦/é æ¸¬é•·åº¦
            input_length = config["initial_input_length"]
            forecast_length = config["initial_forecast_length"]
            # è·³éä¸€å®šå€é–“
            start_index += config["initial_input_length"]

        start_index += 1

    # *** æ–°å¢ï¼šéæ¿¾æ‰å°šæœªå®Œæˆçš„ group
    filtered_X_list = []
    filtered_y_list = []
    filtered_mask_list = []
    filtered_group_id_list = []
    filtered_y_mask_list = []

    for X_item, y_item, m_item, gid_item, y_mask_item in zip(X_list, y_list, mask_list, group_id_list, y_mask_list):
        if gid_item <= last_completed_group_id:
            filtered_X_list.append(X_item)
            filtered_y_list.append(y_item)
            filtered_mask_list.append(m_item)
            filtered_group_id_list.append(gid_item)
            filtered_y_mask_list.append(y_mask_item)

    # === pad X & mask ===
    X_padded = pad_sequences(filtered_X_list, maxlen=max_input_length, dtype='float32', padding='pre', value=0.0)
    mask_padded = pad_sequences(filtered_mask_list, maxlen=max_input_length, dtype='float32', padding='pre', value=0.0)

    # === pad y (è·Ÿä½ åŸæœ¬ç›¸åŒ) ===

    max_forecast_length = config["initial_forecast_length"]
    y_padded = pad_sequences(filtered_y_list, maxlen=max_forecast_length, dtype='float32', padding='post', value=0.0)

    # === pad y_mask (è·Ÿ y ä¸€æ¨£çš„é•·åº¦) ===
    y_mask_padded = pad_sequences(filtered_y_mask_list, maxlen=max_forecast_length, dtype='float32', padding='post',
                                  value=0.0)
    # y_mask_padded.shape = (num_samples, max_forecast_length)

    group_ids = np.array(filtered_group_id_list, dtype=np.int32)

    # ä¾ group_id åˆ†å‰² train/test
    unique_groups = np.unique(group_ids)
    num_groups = len(unique_groups)
    cut_idx = int(num_groups * config["train_ratio"])

    train_groups = unique_groups[:cut_idx]
    test_groups = unique_groups[cut_idx:]

    train_mask = np.isin(group_ids, train_groups)
    test_mask = np.isin(group_ids, test_groups)

    X_train = X_padded[train_mask]
    y_train = y_padded[train_mask]
    mask_train = mask_padded[train_mask]
    group_train = group_ids[train_mask]

    X_test = X_padded[test_mask]
    y_test = y_padded[test_mask]
    mask_test = mask_padded[test_mask]
    group_test = group_ids[test_mask]

    # train
    y_mask_train = y_mask_padded[train_mask]
    ...
    # test
    y_mask_test = y_mask_padded[test_mask]


    return (X_train, y_train, mask_train, y_mask_train, group_train), (X_test, y_test, mask_test, y_mask_test, group_test)




# -------------------- 4. å¯è¦–åŒ– (ç°¡æ˜“æ‰“å°) --------------------
def visualize_data(X, y, mask, group_ids, num_samples=3):
    num_samples = min(num_samples, X.shape[0])
    for i in range(num_samples):
        gid = group_ids[i]
        input_length = int(mask[i].sum())  # è¨ˆç®—æœ‰æ•ˆè¼¸å…¥é•·åº¦

        print(f"\nğŸ“Œ **æ¨£æœ¬ {i + 1}** (Group={gid}, æœ‰æ•ˆè¼¸å…¥é•·åº¦: {input_length})")
        # åªæ‰“å° Close (X[...,3])
        input_values = X[i, :, 3].astype(int)
        output_values = y[i].astype(int)
        print("ğŸ”¹ è¼¸å…¥ (éƒ¨ä»½):", input_values.tolist())
        print("ğŸ”¹ è¼¸å‡º:", output_values.tolist())


def save_data_numpy(X_data, y_data, mask_data, y_mask_data, group_ids, save_path):
    np.savez_compressed(
        save_path,
        X_data=X_data,
        y_data=y_data,
        mask_data=mask_data,
        y_mask_data=y_mask_data,
        group_ids=group_ids
    )
    print(f"âœ… å·²å„²å­˜è‡³ {save_path}")



# -------------------- 5. ä¸»ç¨‹å¼ --------------------
if __name__ == "__main__":
    # è®€å–åŸå§‹æ•¸æ“š
    df = load_mt5_data(config["db_path"], config["table_name"])
    print("åŸå§‹è³‡æ–™ç­†æ•¸:", len(df))

    # ç”¢ç”Ÿ (X, y, mask) + åˆ†ç¾¤ + train/test split
    (X_train, y_train, m_train,y_mask_train, g_train), (X_test, y_test, m_test, y_mask_test, g_test) = prepare_data_and_split(df, config)

    print("\n[Train] X shape:", X_train.shape, "y shape:", y_train.shape, "mask shape:", m_train.shape)
    print("[Test]  X shape:", X_test.shape,  "y shape:", y_test.shape,  "mask shape:", m_test.shape)
    print("Train groupæ•¸:", len(np.unique(g_train)))
    print("Test groupæ•¸:", len(np.unique(g_test)))

    # ç°¡å–®æª¢æŸ¥
    print("\n=== Trainé›†(å‰å¹¾ç­†) ===")
    visualize_data(X_train, y_train, m_train, g_train, num_samples=config["num_samples_to_visualize"])

    # åˆ†åˆ¥å„²å­˜
    save_data_numpy(X_train, y_train, m_train, y_mask_train, g_train, "train_raw.npz")
    save_data_numpy(X_test, y_test, m_test, y_mask_test, g_test, "test_raw.npz")

    print("å…¨éƒ¨è™•ç†å®Œç•¢!")
