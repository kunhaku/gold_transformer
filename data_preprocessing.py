import sqlite3
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.sequence import pad_sequences

### -------------------- 1. åƒæ•¸é…ç½® --------------------
config = {
    "db_path": "mt5_data.db",  # SQLite æ•¸æ“šåº«è·¯å¾‘
    "table_name": "main.XAUUSD",  # æ•¸æ“šè¡¨åç¨±
    "initial_input_length": 20,  # æœ€çŸ­çš„è¼¸å…¥é•·åº¦
    "max_input_length": 30,  # æœ€å¤§çš„è¼¸å…¥é•·åº¦
    "initial_forecast_length": 10,  # æœ€åˆçš„é æ¸¬é•·åº¦
    "min_forecast_length": 3,  # æœ€å°çš„é æ¸¬é•·åº¦
    "num_samples_to_visualize": 10  # å¯è¦–åŒ–çš„æ¨£æœ¬æ•¸
}


### -------------------- 2. è®€å–æ•¸æ“š --------------------
def load_mt5_data(db_path, table_name=config["table_name"]):
    """
    å¾ SQLite æ•¸æ“šåº«è®€å– OHLCV æ•¸æ“šï¼Œè½‰æ›ç‚º Pandas DataFrameã€‚

    Args:
        db_path (str): æ•¸æ“šåº«æ–‡ä»¶è·¯å¾‘
        table_name (str): OHLCV æ•¸æ“šè¡¨åç¨±

    Returns:
        pd.DataFrame: è½‰æ›å¾Œçš„ OHLCV æ•¸æ“š
    """
    conn = sqlite3.connect(db_path)
    query = f"SELECT time, open, high, low, close, tick_volume FROM {table_name} ORDER BY time ASC"
    df = pd.read_sql(query, conn)
    conn.close()

    # è½‰æ›æ™‚é–“æ ¼å¼
    df['time'] = pd.to_datetime(df['time'])
    return df


### -------------------- 3. é è™•ç†æ•¸æ“š --------------------
def prepare_data(df, config):
    """
    ç”Ÿæˆè¨“ç·´æ•¸æ“šï¼Œå‹•æ…‹è®ŠåŒ–è¼¸å…¥é•·åº¦èˆ‡é æ¸¬é•·åº¦ï¼Œä¸¦æ‡‰ç”¨ Maskingã€‚
    ç•¶é æ¸¬é•·åº¦ (`forecast_length`) é”åˆ° `min_forecast_length`ï¼Œå°‡ `input_length` é‡ç½®ä¸¦é–‹å§‹æ–°çš„æ»‘å‹•çª—å£ã€‚

    Args:
        df (pd.DataFrame): åŒ…å« OHLCV æ•¸æ“šçš„ DataFrameã€‚
        config (dict): è¶…åƒæ•¸å­—å…¸ã€‚

    Returns:
        X_data: å½¢ç‹€ç‚º (num_samples, max_input_length, 5) çš„è¼¸å…¥æ•¸æ“š (å¡«å……)ã€‚
        y_data: å½¢ç‹€ç‚º (num_samples, max_forecast_length) çš„ç›®æ¨™æ•¸æ“šã€‚
        mask_data: å½¢ç‹€ç‚º (num_samples, max_input_length, 1) çš„ Masking çŸ©é™£ã€‚
    """
    X_list, y_list, mask_list = [], [], []

    data = df[['open', 'high', 'low', 'close', 'tick_volume']].values
    total_length = len(data)

    # è®€å–åƒæ•¸
    input_length = config["initial_input_length"]
    forecast_length = config["initial_forecast_length"]
    max_input_length = config["max_input_length"]
    min_forecast_length = config["min_forecast_length"]
    max_forecast_length = forecast_length  # è¨˜éŒ„æœ€å¤§é æ¸¬é•·åº¦

    start_index = 0

    while start_index + input_length + forecast_length <= total_length:
        X = data[start_index:start_index + input_length]  # å–å¾—è¼¸å…¥æ•¸æ“š
        y = data[start_index + input_length : start_index + input_length + forecast_length, 3]  # å–å¾—å®Œæ•´é æ¸¬åºåˆ—

        # ç”Ÿæˆ Mask
        mask = np.ones((input_length, 1))

        X_list.append(X)
        y_list.append(y)
        mask_list.append(mask)

        # èª¿æ•´è¼¸å…¥é•·åº¦ & é æ¸¬é•·åº¦
        if forecast_length > min_forecast_length:
            input_length += 1  # å¢åŠ è¼¸å…¥é•·åº¦
            forecast_length -= 1  # é æ¸¬é•·åº¦æ¸›å°‘
        else:
            input_length = config["initial_input_length"]  # é‡ç½®è¼¸å…¥é•·åº¦
            forecast_length = config["initial_forecast_length"]  # é‡ç½®é æ¸¬é•·åº¦
            start_index += config["initial_input_length"]  # è·³éä¸€å®šå€é–“ï¼Œé¿å…éåº¦é‡ç–Š

        start_index += 1  # æ»‘å‹•çª—å£å‰ç§»

    # ä½¿ç”¨ pad_sequences é€²è¡Œå¡«å……
    X_padded = pad_sequences(X_list, maxlen=max_input_length, dtype='float32', padding='pre', value=0.0)
    mask_padded = pad_sequences(mask_list, maxlen=max_input_length, dtype='float32', padding='pre', value=0.0)
    y_padded = pad_sequences(y_list, maxlen=max_forecast_length, dtype='float32', padding='post', value=0.0)

    return X_padded, y_padded, mask_padded



### -------------------- 4. å¯è¦–åŒ–æ•¸æ“š --------------------
import matplotlib.pyplot as plt
import numpy as np


def visualize_data(X, y, mask, num_samples=3):
    """
    ä»¥æ•¸å­—é™£åˆ—æ–¹å¼é¡¯ç¤ºå‰ num_samples çµ„æ•¸æ“šï¼ŒåŒ…æ‹¬:
    - è¼¸å…¥ (X)
    - é æ¸¬ (y)

    Args:
        X (np.array): é è™•ç†å¾Œçš„è¼¸å…¥æ•¸æ“šã€‚
        y (np.array): é æ¸¬ç›®æ¨™ Close åƒ¹æ ¼ (å®Œæ•´é æ¸¬åºåˆ—)ã€‚
        mask (np.array): Masking çŸ©é™£ã€‚
        num_samples (int): é¡¯ç¤ºçš„æ¨£æœ¬æ•¸ (é è¨­å‰3çµ„)ã€‚
    """
    num_samples = min(num_samples, X.shape[0])  # ç¢ºä¿ä¸è¶…éè³‡æ–™ç¸½æ•¸

    print("\n=== è³‡æ–™å¯è¦–åŒ– (æ•¸å­—æ ¼å¼) ===")

    for i in range(num_samples):
        input_length = int(mask[i].sum())  # è¨ˆç®—æœ‰æ•ˆè¼¸å…¥é•·åº¦
        print(f"\nğŸ“Œ **æ¨£æœ¬ {i + 1}** (è¼¸å…¥é•·åº¦: {input_length})")

        # å–å¾—è¼¸å…¥æ•¸æ“š (åªå– close åƒ¹æ ¼ï¼Œä¸¦å°é½Š padding)
        input_values = X[i, :, 3].astype(int)  # å– Close åƒ¹æ ¼ä¸¦è½‰æ›ç‚ºæ•´æ•¸æ–¹ä¾¿é–±è®€
        print("ğŸ”¹ **è¼¸å…¥:**")
        print(input_values.tolist())

        # å–å¾—é æ¸¬æ•¸æ“š (å°é½Š padding)
        output_values = y[i, :].astype(int)  # è½‰æ›ç‚ºæ•´æ•¸
        print("ğŸ”¹ **è¼¸å‡º:**")
        print(output_values.tolist())

    print("\n=== é¡¯ç¤ºçµæŸ ===\n")

def save_data_numpy(X_data, y_data, mask_data, save_path="dataset.npz"):
    """
    ä»¥ NumPy `.npz` æ ¼å¼å„²å­˜æ•¸æ“šï¼Œæ–¹ä¾¿ TensorFlow è®€å–ã€‚

    Args:
        X_data (np.array): è¼¸å…¥æ•¸æ“šã€‚
        y_data (np.array): é æ¸¬ç›®æ¨™æ•¸æ“šã€‚
        mask_data (np.array): Masking çŸ©é™£ã€‚
        save_path (str): å„²å­˜çš„æ–‡ä»¶åç¨±ã€‚
    """
    np.savez_compressed(save_path, X_data=X_data, y_data=y_data, mask_data=mask_data)
    print(f"âœ… æ•¸æ“šå·²æˆåŠŸå„²å­˜è‡³ {save_path}")


### -------------------- 5. ä¸»ç¨‹å¼ --------------------
if __name__ == "__main__":
    # è®€å–æ•¸æ“š
    df = load_mt5_data(config["db_path"], config["table_name"])

    # é è™•ç†æ•¸æ“š
    X_data, y_data, mask_data = prepare_data(df, config)

    # é¡¯ç¤ºæ•¸æ“šå½¢ç‹€
    print(f"X_data shape: {X_data.shape}")
    print(f"y_data shape: {y_data.shape}")
    print(f"mask_data shape: {mask_data.shape}")

    # **æ›´æ–°: ä½¿ç”¨æ–°çš„å¯è¦–åŒ–å‡½æ•¸**
    visualize_data(X_data, y_data, mask_data, num_samples=config["num_samples_to_visualize"])
    # åŸ·è¡Œå„²å­˜
    save_data_numpy(X_data, y_data, mask_data)

