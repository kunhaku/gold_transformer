import sqlite3
import numpy as np
import pandas as pd
from tensorflow.keras.preprocessing.sequence import pad_sequences

# -------------------- 1. åƒæ•¸é…ç½® --------------------
config = {
    "db_path": "mt5_data.db",        # SQLite æ•¸æ“šåº«è·¯å¾‘
    "table_name": "main.XAUUSD",      # æ•¸æ“šè¡¨åç¨±
    "initial_input_length": 36,       # æœ€çŸ­çš„è¼¸å…¥é•·åº¦
    "max_input_length": 48,           # æœ€å¤§çš„è¼¸å…¥é•·åº¦ (padding ç”¨)
    "initial_forecast_length": 24,    # æœ€åˆçš„é æ¸¬é•·åº¦
    "min_forecast_length": 10,         # æœ€å°çš„é æ¸¬é•·åº¦
    "num_samples_to_visualize": 5,    # å¯è¦–åŒ–çš„æ¨£æœ¬æ•¸
    "train_ratio": 0.8,               # è¨“ç·´é›†ä½”æ¯” (ç¾¤çµ„å±¤é¢)
    # å®šç¾©åŸå§‹ç‰¹å¾µèˆ‡æ–°å¢çš„æŒ‡æ¨™ï¼ˆä¾ config é †åºæ±ºå®šæœ€çµ‚çš„ç‰¹å¾µæ’åˆ—ï¼‰
    "base_features": ["open", "high", "low", "close", "tick_volume"],
    "additional_indicators": [
        "oc_dist", "oh_dist", "hl_dist", "lc_dist",
        "RSI", "MA3", "MA12", "MA_diff",
        "boll_upper", "boll_lower", "boll_bandwidth"
    ]
}


# -------------------- 2. è®€å–æ•¸æ“šä¸¦è¨ˆç®—æŒ‡æ¨™ --------------------
def load_mt5_data(db_path, table_name):
    """
    å¾ SQLite æ•¸æ“šåº«è®€å– OHLCV æ•¸æ“šï¼Œè½‰æ›ç‚º Pandas DataFrameï¼Œ
    ä¸¦è¨ˆç®—é¡å¤–æŒ‡æ¨™ï¼ˆRSIã€MAã€å¸ƒæ—å¸¶ç­‰ï¼‰ã€‚
    """
    conn = sqlite3.connect(db_path)
    query = f"SELECT time, open, high, low, close, tick_volume FROM {table_name} ORDER BY time ASC"
    df = pd.read_sql(query, conn)
    conn.close()

    # è½‰æ›æ™‚é–“æ ¼å¼
    df['time'] = pd.to_datetime(df['time'])

    # è¨ˆç®— K æ£’è·é›¢æŒ‡æ¨™ï¼ˆä¿ç•™åŸæœ‰ï¼‰
    df['oc_dist'] = df['close'] - df['open']
    df['oh_dist'] = df['high'] - df['open']
    df['hl_dist'] = df['high'] - df['low']
    df['lc_dist'] = df['close'] - df['low']

    # RSI (æ¡ç”¨ window=14)
    window = 14
    delta = df['close'].diff()
    gain = delta.clip(lower=0)
    loss = -delta.clip(upper=0)
    avg_gain = gain.rolling(window=window, min_periods=window).mean()
    avg_loss = loss.rolling(window=window, min_periods=window).mean()
    rs = avg_gain / avg_loss
    df['RSI'] = 100 - (100 / (1 + rs))

    # MA æŒ‡æ¨™ï¼šMA3 èˆ‡ MA12ï¼Œä¸¦è¨ˆç®—å·®å€¼
    df['MA3'] = df['close'].rolling(window=3, min_periods=3).mean()
    df['MA12'] = df['close'].rolling(window=12, min_periods=12).mean()
    df['MA_diff'] = df['MA3'] - df['MA12']

    # å¸ƒæ—å¸¶ï¼šä»¥ MA12 ç‚ºä¸­è»Œï¼Œæ¡ç”¨1.8æ¨™æº–å·®è¨ˆç®—ä¸Šè»Œèˆ‡ä¸‹è»Œ
    df['boll_std'] = df['close'].rolling(window=12, min_periods=12).std()
    df['boll_upper'] = df['MA12'] + 1.8 * df['boll_std']
    df['boll_lower'] = df['MA12'] - 1.8 * df['boll_std']
    # å¸ƒæ—å¸¶å¯¬åº¦ï¼ˆå¯æ­¸ä¸€åŒ–ï¼‰
    df['boll_bandwidth'] = (df['boll_upper'] - df['boll_lower']) / df['MA12']

    # ç§»é™¤å›  rolling è¨ˆç®—è€Œç”¢ç”Ÿçš„ NaN
    df.dropna(inplace=True)
    df.reset_index(drop=True, inplace=True)
    return df


# -------------------- 3. ç”Ÿæˆè³‡æ–™ + åˆ†ç¾¤ + åˆ†å‰² --------------------
def prepare_data_and_split(df, config):
    """
    1) æŒ‰ç…§å‹•æ…‹çª—å£ç”¢ç”Ÿ (X, y, mask)
    2) çµ¦æ¯ç­†æ¨£æœ¬ä¸€å€‹ group_id (ä¸¦ç”¢ç”Ÿ y_mask)
    3) è‹¥æœ€å¾Œä¸€çµ„æœªå®Œæˆï¼Œå‰‡æ’é™¤è©²ç¾¤çµ„
    4) ä¾ group_id åˆ‡åˆ† train/test
    """
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    # ä¾ config å®šç¾©æœ€çµ‚ç‰¹å¾µé †åº
    features = config["base_features"] + config["additional_indicators"]

    X_list, y_list, mask_list, group_id_list = [], [], [], []
    y_mask_list = []

    # å–å‡ºæŒ‡å®šæ¬„ä½ï¼Œæ³¨æ„æ¬„ä½é †åºå¿…é ˆèˆ‡ config["features"] ä¸€è‡´
    data = df[features].values  # shape = (total_length, num_features)
    total_length = len(data)

    input_length = config["initial_input_length"]
    forecast_length = config["initial_forecast_length"]
    max_input_length = config["max_input_length"]
    min_forecast_length = config["min_forecast_length"]

    start_index = 0
    current_group_id = 0
    last_completed_group_id = -1

    while start_index + input_length + forecast_length <= total_length:
        X = data[start_index: start_index + input_length]
        # ä»¥ close ç‚ºç›®æ¨™ï¼Œclose åœ¨ features è£¡é¢ä½æ–¼ç¬¬ 4 (è‹¥å¾0è¨ˆç®— index=3)
        y = data[start_index + input_length: start_index + input_length + forecast_length, 3]
        mask = np.ones((input_length, 1))
        y_mask_unpadded = np.ones((forecast_length,), dtype='float32')

        X_list.append(X)
        y_list.append(y)
        mask_list.append(mask)
        group_id_list.append(current_group_id)
        y_mask_list.append(y_mask_unpadded)

        if forecast_length > min_forecast_length:
            input_length += 1
            forecast_length -= 1
        else:
            last_completed_group_id = current_group_id
            current_group_id += 1
            input_length = config["initial_input_length"]
            forecast_length = config["initial_forecast_length"]
            start_index += config["initial_input_length"]

        start_index += 1

    filtered_X_list, filtered_y_list, filtered_mask_list, filtered_group_id_list, filtered_y_mask_list = [], [], [], [], []
    for X_item, y_item, m_item, gid_item, y_mask_item in zip(X_list, y_list, mask_list, group_id_list, y_mask_list):
        if gid_item <= last_completed_group_id:
            filtered_X_list.append(X_item)
            filtered_y_list.append(y_item)
            filtered_mask_list.append(m_item)
            filtered_group_id_list.append(gid_item)
            filtered_y_mask_list.append(y_mask_item)

    X_padded = pad_sequences(filtered_X_list, maxlen=max_input_length, dtype='float32', padding='pre', value=0.0)
    mask_padded = pad_sequences(filtered_mask_list, maxlen=max_input_length, dtype='float32', padding='pre', value=0.0)
    max_forecast_length = config["initial_forecast_length"]
    y_padded = pad_sequences(filtered_y_list, maxlen=max_forecast_length, dtype='float32', padding='post', value=0.0)
    y_mask_padded = pad_sequences(filtered_y_mask_list, maxlen=max_forecast_length, dtype='float32', padding='post', value=0.0)
    group_ids = np.array(filtered_group_id_list, dtype=np.int32)

    unique_groups = np.unique(group_ids)
    num_groups = len(unique_groups)
    cut_idx = int(num_groups * config["train_ratio"])

    train_groups = unique_groups[:cut_idx]
    test_groups = unique_groups[cut_idx:]

    train_mask = np.isin(group_ids, train_groups)
    test_mask = np.isin(group_ids, test_groups)

    X_train = X_padded[train_mask]
    y_train = y_padded[train_mask]
    mask_train = mask_padded[train_mask]
    y_mask_train = y_mask_padded[train_mask]
    group_train = group_ids[train_mask]

    X_test = X_padded[test_mask]
    y_test = y_padded[test_mask]
    mask_test = mask_padded[test_mask]
    y_mask_test = y_mask_padded[test_mask]
    group_test = group_ids[test_mask]

    return (X_train, y_train, mask_train, y_mask_train, group_train), (X_test, y_test, mask_test, y_mask_test, group_test)


# -------------------- 4. å¯è¦–åŒ– (ç°¡æ˜“æ‰“å°) --------------------
def visualize_data(X, y, mask, group_ids, num_samples=3):
    num_samples = min(num_samples, X.shape[0])
    for i in range(num_samples):
        gid = group_ids[i]
        input_length = int(mask[i].sum())
        print(f"\nğŸ“Œ æ¨£æœ¬ {i+1} (Group={gid}, æœ‰æ•ˆè¼¸å…¥é•·åº¦: {input_length})")
        input_values = X[i, :, 3].astype(int)  # é€™è£¡é¡¯ç¤º close å€¼
        output_values = y[i].astype(int)
        print("ğŸ”¹ è¼¸å…¥:", input_values.tolist())
        print("ğŸ”¹ è¼¸å‡º:", output_values.tolist())


def save_data_numpy(X_data, y_data, mask_data, y_mask_data, group_ids, save_path):
    np.savez_compressed(save_path,
                        X_data=X_data,
                        y_data=y_data,
                        mask_data=mask_data,
                        y_mask_data=y_mask_data,
                        group_ids=group_ids)
    print(f"âœ… å·²å„²å­˜è‡³ {save_path}")


# -------------------- 5. ä¸»ç¨‹å¼ --------------------
if __name__ == "__main__":
    df = load_mt5_data(config["db_path"], config["table_name"])
    print("åŸå§‹è³‡æ–™ç­†æ•¸:", len(df))
    (X_train, y_train, mask_train, y_mask_train, g_train), (X_test, y_test, mask_test, y_mask_test, g_test) = prepare_data_and_split(df, config)
    print("\n[Train] X shape:", X_train.shape, "y shape:", y_train.shape, "mask shape:", mask_train.shape)
    print("[Test]  X shape:", X_test.shape, "y shape:", y_test.shape, "mask shape:", mask_test.shape)
    print("Train groupæ•¸:", len(np.unique(g_train)))
    print("Test groupæ•¸:", len(np.unique(g_test)))
    print("\n=== Trainé›†(å‰å¹¾ç­†) ===")
    visualize_data(X_train, y_train, mask_train, g_train, num_samples=config["num_samples_to_visualize"])
    save_data_numpy(X_train, y_train, mask_train, y_mask_train, g_train, "train_raw.npz")
    save_data_numpy(X_test, y_test, mask_test, y_mask_test, g_test, "test_raw.npz")
    print("å…¨éƒ¨è™•ç†å®Œç•¢!")
